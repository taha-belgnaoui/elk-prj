{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Option Pricing, Master 2 Probabilités et Finance, Sorbonne Université and École polytechnique\n",
    "# Homework I (optional)\n",
    "\n",
    "### Due Date: 1:55 PM Tuesday, January 21, 2025\n",
    "You should turn in the notebook on the Moodle course website.\n",
    "\n",
    "Please comment your code properly.\n",
    "\n",
    "Before you turn in the notebook, press the \"Run all cells\" button in the toolbar, and make sure all the calculation results and graphs are produced correctly in a reasonable time frame, and then save the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlibNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.6/8.0 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.0 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 16.6 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Installing collected packages: contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 matplotlib-3.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\tahab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "plt.rc('figure', figsize=(6, 5.5))\n",
    "plt.rc('axes', grid=True, xmargin=0, ymargin=0, autolimit_mode='round_numbers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Conditional Expection and Least Square Regression\n",
    "\n",
    "Let $X$ and $Y$ be two random variables. The conditional expectation $\\mathbb{E}\\left[Y|X\\right]$ is a function $f^{\\ast}$ of $X$ that best approximates $Y$ in the least square sense, i.e.,\n",
    "\n",
    "$$\\mathbb{E}\\left[Y|X\\right]=f^{\\ast}(X)\\quad\\text{and}\\quad\\mathbb{E}\\left[\\left|Y-f^{\\ast}(X)\\right|^2\\right]\\leq\\mathbb{E}\\left[\\left|Y-f(X)\\right|^2\\right]\\text{ for any function }f\\text{ of }X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the random variables $X$ and $Y$ by\n",
    "$$\n",
    "g(x) = x \\frac{1 + x}{1 + x^2}, \\qquad X \\sim \\mathcal{N}(0, 1), \\quad Y = g(X) + \\varepsilon\n",
    "$$\n",
    "where $\\varepsilon \\sim \\mathcal{N}(0, 1/16)$ is independent of $X$.\n",
    "\n",
    "Note that $\\mathbb{E}\\left[\\left.Y\\right|X\\right] = \\mathbb{E}\\left[\\left.g(X)+\\varepsilon\\right|X\\right] = g(X)+\\mathbb{E}\\left[\\left.\\varepsilon\\right|X\\right] = g(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random sample of the joint distribution and the \n",
    "# theoretical conditional expectation of Y wrt X.\n",
    "\n",
    "def g(x):\n",
    "    return x*(1+x)/(1+x**2)\n",
    "\n",
    "n = 200\n",
    "sigma = 0.25\n",
    "X = np.random.randn(n)\n",
    "Y = g(X) + sigma * np.random.randn(n)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, alpha=0.5)\n",
    "x = np.linspace(X.min(), X.max(), 101)\n",
    "ax.plot(x, g(x), 'k', linewidth=2)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric regression\n",
    "\n",
    "The conditional expection $\\mathbb{E}\\left[\\left.Y\\right|X\\right]$ is approximated by a linear combination of a set of given <em>basis</em> functions $\\{f_i(X)\\}_{0\\leq i\\leq n}$, i.e.,\n",
    "\n",
    "$$\\mathbb{E}\\left[Y|X\\right]\\approx \\beta_0^*f_0(X)+\\cdots+\\beta_n^*f_n(X)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mathbb{E}\\left[\\left(Y-\\beta_0^*f_0(X)-\\cdots-\\beta_n^*f_n(X)\\right)^2\\right] = \\min_{\\beta_1,\\ldots,\\beta_n}\\mathbb{E}\\left[\\left(Y-\\beta_0f_0(X)-\\cdots-\\beta_nf_n(X)\\right)^2\\right]$$\n",
    "\n",
    "Given $N$ observations $\\left((x_1,y_1),\\ldots,(x_N, y_N)\\right)$ of $X$ and $Y$, one finds the optimal parameters $\\beta_i$ by solving the least square problem $\\min_\\beta\\|A\\beta-y\\|_2$, where\n",
    "\n",
    "$$A=\\begin{bmatrix}f_0(x_1) & \\cdots & f_n(x_1)\\\\ \\vdots & \\ddots & \\vdots \\\\ f_0(x_N) & \\cdots & f_n(x_N)\\end{bmatrix}\\quad\n",
    "\\text{and}\\quad y=\\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_N\\end{bmatrix}$$\n",
    "\n",
    "The numpy routine <strong>numpy.linalg.lstsq</strong> can be used to solve such linear least square problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomials\n",
    "\n",
    "The basis functions are taken to be power functions $f_i(X)=X^{i}$, that is $\\mathbb{E}\\left[Y|X\\right]$ is a polynomial of $X$. The numpy routine <strong>numpy.polyfit</strong> is a convenient way to obtain the least square polynomial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial of degree 3 to the sample points (X, Y)\n",
    "p = np.polyfit(X, Y, deg=3)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, alpha=0.5)\n",
    "ax.plot(x, np.polyval(p, x), 'r', lw=2, label='Polynomial Fit')\n",
    "ax.plot(x, g(x), 'k', label='True Function')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwlin_basis(xknots):\n",
    "    \"\"\"Basis that represent a piecewise linear function with given knots\"\"\"\n",
    "    fs = [lambda x: np.ones_like(x, dtype=np.float), lambda x: x-xknots[0]]\n",
    "    fs.extend([lambda x, a=xknots[i]: np.maximum(x-a, 0) for i in range(len(xknots))])\n",
    "    return fs\n",
    "\n",
    "def pwlin_fit(xdata, ydata, xknots):\n",
    "    \"\"\"Fit a piecewise linear function with xknots to xdata and ydata\"\"\"\n",
    "    fs = pwlin_basis(xknots)\n",
    "    A = np.column_stack([f(xdata) for f in fs])\n",
    "    ps = np.linalg.lstsq(A, ydata, rcond=None)[0]\n",
    "    return ps, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xknots = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 8)\n",
    "ps, fs = pwlin_fit(X, Y, xknots)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, alpha=0.5)\n",
    "ax.plot(x, sum([f(x)*p for (f, p) in zip(fs, ps)]), 'r', label='Piecewise Linear Fit')\n",
    "ax.plot(x, g(x), 'k', label='True Function')\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric regression\n",
    "\n",
    "### Nadaraya-Watson Kernel regression (Local Weighted Average)\n",
    "\n",
    "Here, no parametric form is assumed for $\\mathbb{E}[Y|X]$. Instead, local averages of $Y$ values are computed, given the value of $X$:\n",
    "\n",
    "$$\\mathbb{E}\\left[Y|X\\right]\\approx\\frac{\\sum_{i=1}^NK_h(x-x_i)y_i}{\\sum_{i=1}^NK_h(x-x_i)}$$\n",
    "where $K$ is a kernel function and $K_h(x)=K(x/h)/h$, $h$ is the <em>bandwidth</em>. $K_h$ approximates the Dirac mass at zero.\n",
    "\n",
    "### Local Linear Regression\n",
    "\n",
    "The locally weighted linear regression solves a separate weighted least squares problem at each target point $x$,\n",
    "\n",
    "$$\\hat{\\alpha},\\hat{\\beta} = \\text{argmin}_{\\alpha,\\beta}\\sum_{i=1}^NK_h(x-x_i)\\left[y_i-\\alpha-\\beta x_i\\right]^2$$\n",
    "\n",
    "which yields an estimate $\\hat{\\alpha}+\\hat{\\beta}x$. Note that $\\hat{\\alpha}$ and $\\hat{\\beta}$ depend on $x$. The locally-weighted averages can be badly biased on the boundaries. This bias\n",
    "can be removed by local linear regression to the first order.\n",
    "\n",
    "<b>Note.</b> To speed up, we often perform the local regression only at a selection of points and then use interpolation/extrapolation to evaluate at other target points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-parametric regression function\n",
    "\n",
    "def gauss_kern(x):\n",
    "    \"\"\"Gaussian kernel function\"\"\"\n",
    "    return np.exp(-x**2/2)\n",
    "\n",
    "def kern_reg(x, xdata, ydata, bandwidth, kern=gauss_kern):\n",
    "    \"\"\"Nadaraya-Watson Kernel Regression (Locally weighted average)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array_like, one-dimensional\n",
    "        The x-coordinates of the target points\n",
    "    xdata: array_like\n",
    "        The x-coordinates of the data points.\n",
    "    ydata: array_like\n",
    "        The y-coordinates of the data points. \n",
    "    bandwidth: positive scalar\n",
    "        Bandwidth of the kernel\n",
    "    kern: callable\n",
    "        kernel function\n",
    "    \"\"\"\n",
    "    weights = kern((xdata[:, np.newaxis] - x) / bandwidth)\n",
    "    return np.sum(weights * ydata[:, np.newaxis], axis=0) / np.sum(weights, axis=0)\n",
    "\n",
    "\n",
    "def ll_reg(x, xdata, ydata, bandwidth, kern=gauss_kern):\n",
    "    \"\"\"Local Linear Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array_like, one-dimensional\n",
    "        The x-coordinates of the target points\n",
    "    xdata: array_like\n",
    "        The x-coordinates of the data points.\n",
    "    ydata: array_like\n",
    "        The y-coordinates of the data points. \n",
    "    bandwidth: positive scalar\n",
    "        Bandwidth of the kernel\n",
    "    kern: callable\n",
    "        kernel function\n",
    "    \"\"\"\n",
    "    \n",
    "    def func(xx):\n",
    "        weights = np.sqrt(kern((xdata-xx)/bandwidth))\n",
    "        b = ydata*weights\n",
    "        A = np.column_stack((np.ones_like(xdata), xdata-xx))*weights[:, np.newaxis]\n",
    "        yy, _ = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "        return yy\n",
    "    \n",
    "    return np.vectorize(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(11, 4.75))\n",
    "bw_silverman = (4/(3*len(X)))**0.2*np.std(X)\n",
    "# NW Kernel Regression\n",
    "xknots0 = np.linspace(np.percentile(X, 2.5), np.percentile(X, 97.5), 20)\n",
    "yknots0 = kern_reg(xknots0, X, Y, bw_silverman, gauss_kern)\n",
    "f0 = interp1d(xknots0, yknots0, kind='linear', fill_value='extrapolate')\n",
    "axs[0].plot(x, f0(x), color='r', label='NW Kernel Reg (bw={:.2f})'.format(bw_silverman))\n",
    "axs[0].set_title('NW Kernel Regression')\n",
    "# Local Linear Regression\n",
    "xknots1 = xknots0\n",
    "yknots1 = ll_reg(xknots1, X, Y, bw_silverman, gauss_kern)\n",
    "f1 = interp1d(xknots1, yknots1, kind='linear', fill_value='extrapolate')\n",
    "axs[1].plot(x, f1(x), color='r', label='Local Linear Reg (bw={:.2f})'.format(bw_silverman))\n",
    "axs[1].set_title('Local Linear Regression')\n",
    "for ax in axs:\n",
    "    ax.scatter(X, Y, alpha=0.5)\n",
    "    ax.plot(x, g(x), 'k', label='True Function')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.1</b>. (Parametric regression) In general, increasing the number of basis functions in the regression gives us more flexibility to better fit the data. However, having too many parameters in the model oftentimes leads to overfitting, which usually has poor predictive performance and is over-sensitive to small noise in the data. To observe the overftting phenomenon, in polynomial fit, try to use different degrees of the polynomials; in piecewise-linear regression, try to use difference number of knots. Then reproduce the scatter plot with fitted regression function. Compare and comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.2</b>. For nonparametric regression,\n",
    "<ul>\n",
    "<li>Try different bandwidth values in the kernel regression. Reproduce the scatter plot with fitted regression function. Compare and comment on the results. For what values of the bandwidth do we observe overfitting? For what values of the bandwidth do we observe a poor fit?</li>\n",
    "<li>Try to use different kernels, for example\n",
    "$$K(x)=(x+1)^2(1-x)^2\\quad\\text{for }-1\\leq x\\leq 1\\quad\\text{and}\\quad0\\quad\\text{ elsewhere.}$$\n",
    "Which has more impact: the bandwidth $h$ or the kernel $K$?\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. American Option Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackscholes_mc(ts, n_paths, S0, vol, r, q):\n",
    "    \"\"\"Generate Monte-Carlo paths in Black-Scholes model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts: array_like\n",
    "        The time steps of the simualtion\n",
    "    n_paths: int\n",
    "        the number of paths to simulate\n",
    "    S0: scalar\n",
    "        The spot price of the underlying security.\n",
    "    vol: scalar\n",
    "        The implied Black-Scholes volatility.\n",
    "    r: scalar\n",
    "        The annualized risk-free interest rate, continuously compounded.\n",
    "    q: scalar\n",
    "        The annualized continuous dividend yield.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths: ndarray\n",
    "        The Monte-Carlo paths.\n",
    "    \"\"\"\n",
    "    paths = np.full((len(ts), n_paths), np.nan, dtype=np.float)\n",
    "    paths[0] = S0\n",
    "    for i in range(len(ts)-1):\n",
    "        dt = ts[i+1] - ts[i]\n",
    "        dW = np.sqrt(dt)*np.random.randn(n_paths)\n",
    "        paths[i+1] = paths[i] * np.exp((r-q-1/2*vol**2)*dt + vol*dW)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100\n",
    "vol = 0.2\n",
    "r = 0.1\n",
    "q = 0.02\n",
    "K = 100\n",
    "T = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0, 1, 13)\n",
    "n_paths = 10000\n",
    "paths = blackscholes_mc(ts, n_paths, S0, vol, r, q)\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "ax.plot(ts, paths[:, :10])\n",
    "ax.set_xticks(ts)\n",
    "ax.set_xticklabels(['0', '1M', '2M', '3M', '4M', '5M', '6M', '7M', '8M', '9M', '10M', '11M', '1Y'])\n",
    "ax.set_ylabel('Stock Price')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price of European put option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.maximum(K-paths[-1], 0))*np.exp(-r*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price of Bermudan put option with monthly exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longstaff-Schwartz algorithm\n",
    "\n",
    "We use polynomials as basis functions for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payoff = np.maximum(K-paths[-1], 0)\n",
    "for i in range(len(ts)-2, 0, -1):\n",
    "    discount = np.exp(-r*(ts[i+1]-ts[i]))\n",
    "    payoff = payoff*discount\n",
    "    p = np.polyfit(paths[i], payoff, deg=2)\n",
    "    contval = np.polyval(p, paths[i])\n",
    "    exerval = np.maximum(K-paths[i], 0)\n",
    "    # identify the paths where we should exercise\n",
    "    ind = exerval > contval\n",
    "    payoff[ind] = exerval[ind]\n",
    "np.mean(payoff*np.exp(-r*(ts[1]-ts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tsitsiklis-van Roy algorithm\n",
    "\n",
    "The price from the TVR algorithm is generally too high because regression errors accumulate in the backward induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.maximum(K-paths[-1], 0)\n",
    "for i in range(len(ts)-2, 0, -1):\n",
    "    discount = np.exp(-r*(ts[i+1]-ts[i]))\n",
    "    p = np.polyfit(paths[i], V*discount, deg=2)\n",
    "    contval = np.polyval(p, paths[i])\n",
    "    exerval = np.maximum(K-paths[i], 0)\n",
    "    V = np.maximum(exerval, contval)\n",
    "np.mean(V)*np.exp(-r*(ts[1]-ts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.1</b>. Adapt the Longstaff-Schwartz and TVR code to use different regression methods:\n",
    "\n",
    "<b>(a).</b> Black-Scholes put prices.\n",
    "<br>\n",
    "Use the constant 1.0 and the Black-Scholes price of a European put option with volatility $\\bar\\sigma = 0.2$ and maturity $T-t$ as the two basis functions at time $t$. For your convenience, the Black-Scholes pricing formula is included in the cell below.\n",
    "\n",
    "<b>(b).</b> Piecewise Linear regression.\n",
    "\n",
    "Note: choose the number of knots appropriately in the regression.</li>\n",
    "\n",
    "<b>(c).</b> Kernel regression with Gaussian kernel.\n",
    "\n",
    "Note: Choose the bandwidth appropriately.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackscholes_price(K, T, S0, vol, r=0, q=0, callput='call'):\n",
    "    \"\"\"Compute the call/put option price in the Black-Scholes model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    K: scalar or array_like\n",
    "        The strike of the option.\n",
    "    T: scalar or array_like\n",
    "        The maturity of the option, expressed in years (e.g. 0.25 for 3-month and 2 for 2 years)\n",
    "    S0: scalar or array_like\n",
    "        The current price of the underlying asset.\n",
    "    vol: scalar or array_like\n",
    "        The implied Black-Scholes volatility.\n",
    "    r: scalar or array_like\n",
    "        The annualized risk-free interest rate, continuously compounded.\n",
    "    q: scalar or array_like\n",
    "        The annualized continuous dividend yield.\n",
    "    callput: str\n",
    "        Must be either 'call' or 'put'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    price: scalar or array_like\n",
    "        The price of the option.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> blackscholes_price(95, 0.25, 100, 0.2, r=0.05, callput='put')\n",
    "    1.5342604771222823\n",
    "    \"\"\"\n",
    "    F = S0*np.exp((r-q)*T)\n",
    "    v = vol*np.sqrt(T)\n",
    "    d1 = np.log(F/K)/v + 0.5*v\n",
    "    d2 = d1 - v\n",
    "    try:\n",
    "        opttype = {'call':1, 'put':-1}[callput.lower()]\n",
    "    except:\n",
    "        raise ValueError('The value of callput must be either \"call\" or \"put\".')\n",
    "    price = opttype*(F*norm.cdf(opttype*d1)-K*norm.cdf(opttype*d2))*np.exp(-r*T)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.2</b>. Both TVR and Longstaff-Schwartz algorithms provide an estimate for the expected payoff from continuation at each exercise date from the cross-sectional information in the simulation using regression. By comparing this estimate with the immediate exercise value, the holder of the option can determine whether to exercise the option or continue. \n",
    "\n",
    "Perform an independent Monte Carlo simulation using this estimate of continuation value from <b>both</b> algorithms as an exercise policy, and estimate the price of the American put. Explain why this estimate is a lower bound. The basis functions for regression can be chosen to be the quadratic polynomial as in the sample code.\n",
    "\n",
    "Show your code and result with at least 100000 simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.3</b> Oftentimes parametric regressions may not produce globally accurate estimates of continuation values for all possible values of state variables. One approach to improve the accuracy is to run regression only in the region where option is in the money. When the option is out of money, one should always choose to continue, therefore there is no need to estimate the continuation value.\n",
    "\n",
    "Modify the Longstaff-Schwartz code to implement this improvement.\n",
    "\n",
    "Show your code and results with the following basis functions:\n",
    "\n",
    "<b>(a).</b> quadratic polynomial basis functions;\n",
    "\n",
    "<b>(b).</b> basis functions given in Question 2.1(a)\n",
    "\n",
    "For each case, identify and plot the exercise and continuation regions at time $t=0.5$. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.4</b>. Use the Longstaff-Schwartz algorithm to price an Bermudan-Asian call option. The underlying security price is assumed to follow the geometric Brownian motion\n",
    "\n",
    "$$\\frac{dS_t}{S_t}=(r-q)dt+\\sigma dW_t$$\n",
    "\n",
    "with $S_0=100$, $r=0$, $q=0$ and $\\sigma=0.2$. The option has a strike of $K=100$ and a final expiration date of $T=1$ and can be exercised every month before the final maturity, i.e.\n",
    "$$t_1=\\frac{1}{12},\\, t_2=\\frac{2}{12},\\, \\ldots,\\, t_{12}=1$$\n",
    "\n",
    "The cashflow from exercising the option at time $t_n$, $n=1,\\ldots,12$, is $\\max\\left(0, A_{t_n}-K\\right)$ with $A_{t_n}=\\frac{1}{n}\\sum_{i=1}^nS_{t_i}$.\n",
    "\n",
    "<b>(a).</b>\n",
    "As the basis functions in the regression at time $t_n$, use the constant 1.0 and the Black-Scholes value of a European call option with strike $K$, maturity $T-t_n$, volatility $\\bar\\sigma = 0.1$, and the spot value\n",
    "$$\n",
    "Z_{t_n}\\equiv \\frac{nA_{t_n}+(12-n)S_{t_n}}{12}\n",
    "$$\n",
    "\n",
    "As in Question 2.4, after you have obtained estimations of continuation values from the Longstaff-Schwartz algorithm, run an independent Monte Carlo simulation with at least 100000 paths to obtain a low-biased price. Explain the use of $Z_{t_n}$ instead of simply $A_{t_n}$ or $S_{t_n}$.\n",
    "\n",
    "<b>(b).</b> Implement a feed-forward neural network to estimate the conditional value at each time step. The network should take both $S_{t_n}$ and $A_{t_n}$ as inputs. As in part (a), after all estimations of continuation values are obtained, run an indepedent Monte Carlo simulation to get a low-biased price.\n",
    "\n",
    "Technical Tips: You should use tensorflow to implement the neural networks. There is a tensorflow example in the test notebook for your reference. For this problem, we recommend the following hyperparameters: 3 hidden layers with 20 neurons and RELU activation on each layer. To control the time of training, we recommend training size of 50000 paths and batch size of 128 in the stochastic gradient descent. You can do the usual training/validation split, and use the validation loss to early-stop the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
